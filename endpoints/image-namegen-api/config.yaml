INFERENCE:
  prompt_id: "001"
  api_port: 8001

OFFLINE_SCRAPER:
  prompt_id: "001"
  llm_model: "gpt_4o_mini"

REDIS:
    host: localhost
    port: 6379
    db: 0

CAAS:
  gpt-4o:
    "max_tokens": 750
    "temperature": 0.5
    "frequency_penalty": 0.02
    "presence_penalty": 0
    "provider": "openai_chat"
    "top_p": 1
    "model": "gpt-4o"
    "retry_count": 2

  gpt-4o-mini:
    "max_tokens": 750
    "temperature": 0.5
    "frequency_penalty": 0.02
    "presence_penalty": 0
    "provider": "openai_chat"
    "top_p": 1
    "model": "gpt-4o-mini"
    "retry_count": 2

  gpt-4-vision-preview:
    "max_tokens": 750
    "temperature": 0.5
    "frequency_penalty": 0.02
    "presence_penalty": 0
    "provider": "openai_chat"
    "top_p": 1
    "model": "gpt-4-vision-preview"
    "retry_count": 2

  gpt-4-vision:
    "max_tokens": 750
    "temperature": 0.5
    "frequency_penalty": 0.02
    "presence_penalty": 0
    "provider": "openai_chat"
    "top_p": 1
    "model": "gpt-4-vision-preview"
    "retry_count": 2